# first_streamlit_app
Streamlit App Developed in Snowflake Badge 2 (Data Applications)

API INTEGRATION
Run this code once per Snowflake Trial Account.

use role accountadmin;
create or replace api integration dora_api_integration
api_provider = aws_api_gateway
api_aws_role_arn = 'arn:aws:iam::321463406630:role/snowflakeLearnerAssumedRole'
enabled = true
api_allowed_prefixes = ('https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora');


GRADER FUNCTION

use role accountadmin;  
create or replace external function demo_db.public.grader(
      step varchar
    , passed boolean
    , actual integer
    , expected integer
    , description varchar)
returns variant
api_integration = dora_api_integration 
context_headers = (current_timestamp,current_account, current_statement) 
as 'https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora/grader'
; 


-- set your worksheet drop lists to the location of your GRADER function
--DO NOT EDIT BELOW THIS LINE - Don't add or take away spaces, do change 'from' to 'FROM' - NO EDITS AT ALL
select GRADER(step,(actual = expected), actual, expected, description) as graded_results from (
SELECT 'DORA_IS_WORKING' as step
 ,(select 223 ) as actual
 ,223 as expected
 ,'Dora is working!' as description
); 



select current_region();

--IT36598
SELECT CURRENT_ACCOUNT();

--Here's code to recreate the "Like a Window" Stage in the DEMO_DB. You can just use the stage from GARDEN_PLANTS if you are using the same Trial account from an earlier workout. 
create stage demo_db.public.like_a_window_into_an_s3_bucket
url = 's3://uni-lab-files';

--And here's a reminder of the syntax for COPY INTO:
copy into my_table_name
from @demo_db.public.like_a_window_into_an_s3_bucket
files = ( 'IF_I_HAD_A_FILE_LIKE_THIS.txt')
file_format = ( format_name='EXAMPLE_FILEFORMAT' );


Every Snowflake account has THE SNOWFLAKE database included in the account
It is good to be aware that another name for "THE SNOWFLAKE database" is "the Account Usage Share." 

no ddl statements on inbound shares
inbound shares import permissions and revoke
inbound shares no updates inserts or deletes

The Account Usage Share is weird because you can't drop or rename it like other inbound shares.

What changes appear on the Shared Data screen because we have dropped the SNOWFLAKE_SAMPLE_DATA database?
Select 2.
Two tiles still appear, but one has a download button.
The database name that appeared on SFC_SAMPLES tile is no longer there.

Which of the following were you able to carry out on the SNOWFLAKE database (The Account Usage Share)?
Unable to rename or drop acct usage share. error - insufficient privs


The real value of consuming shared data is:
Someone else will maintain it over time and keep it fresh
Someone else will pay to store it
You will only pay to query it

What are the names of the 3 Snowflake Sharing Technologies?
Direct Sharing
Snowflake Data Marketplace
Private Exchanges

Why do you think the SNOWFLAKE and SNOWFLAKE_SAMPLE_DATA databases did not show up on the list during your share creation process?
Select 2.

Because I don't have the rights to share the data.
Because you can't share a share


Why do you think the interface does not show the views you created?
Because we created "regular" views instead of "secure" views.


Who could you share your new Outbound Share with?
a) I could share it with Martín Rojas at World Data Emporium (SE59172).
b) I could share it with any account on Azure Canada Central (Toronto) -- if they told me their Account Locator.
c) I could share it with other people taking this course - if they told me their Account Locator.
d) I could share it with anyone, even people with no Snowflake Account -- if I set them up with something called a "Reader Account"


Why couldn't the view that includes the NATIONS table be added to the share?
Because the NATIONS table comes from an Inbound Share (SNOWFLAKE_SAMPLE_DATA)
Because you can't share a share.


If thousands of learners who go through this workshop, and all of them share their Outbound Shares with Martín, will Martín's Database list be impossible to navigate?
No, because Inbound shares (for Martín they are inbound) only show on the list of databases if Martín decides to "get" them.


We often use the words ACCOUNT and USER as if they are interchangeable words. In Snowflake, it is important to keep them separate. Look at the statements below and select the ones that are TRUE.
Select 4.
A Snowflake Trial Account is an ACCOUNT, not a USER.
The name and password entered during the login process is a USER, not an ACCOUNT.
When a USER is created, it provides access to a single ACCOUNT, but an ACCOUNT can have many USERS.
Each learner in this Workshop should have a USER they use to log in to a Snowflake Trial ACCOUNT.


Which of the following are true about sub-accounts (also called Managed Accounts or Reader Accounts)?
Select 4.
I can create a "sub-account" to my Snowflake Trial Account. This is different than just creating a Login/User for my Trial Snowflake Account.
When I create the sub-account it gets its own Account Locator and URL.
I should note both of my account URLs so I can recall which is my trial account and which is the sub-account I manage.
I can create logins or USERS for the sub-account. If I don't write down the User and Password I create for my sub-account login, I don't have any way to retrieve it.



How is the managed sub-account different than the trial account?
There's no SNOWFLAKE_SAMPLE_DATA database set up in advance.
There's no COMPUTE_WH warehouse set up in advance.





structured unstructured semi
any data file with data arranged in rows and columns (and no nesting) is called "structured data" and we often load each value into a column and each line into a row in our Snowflake tables
Semi-Structured data (in a file like .json, or .xml)  is data that has unpredictable levels of nesting and often each "row" of data can vary widely in the information it contains. Semi-structured data stored in a flat file may have markup using angle brackets (like XML) or curly brackets (like JSON). Snowflake can load and process common nested data types like JSON, XML, Avro, Parquet, and ORC. Each of the semi-structured data types is loaded into Snowflake tables using a column data type called VARIANT.  A VARIANT column might contain many key-value pairs and multiple nested records and it will also keep the markup symbols like curly braces and angle brackets. 
There is a third type of data file called Unstructured data. (File names will have extensions like .mp3, .mp4, .png, etc.)  Snowflake added support for Unstructured Data in August of 2021. Snowflake has ways to help you store, access, process, manage, govern, and share unstructured data. Videos, images, audio files, pdfs and other file types are considered unstructured data.



We already know that in the wider world of Data Warehousing, we can use the word "stage" to mean "a temporary storage location", and we can also use "stage" to mean a cloud folder where data is stored -- but now, more than ever, we should open our mind to the idea that a defined Snowflake Stage Object is most accurately thought of as a named gateway into a cloud folder where, presumably, data files are stored either short OR long term. 

Name the 3 structural data types Snowflake can manage.
List the 5 semi-structured data types Snowflake can load into VARIANT columns in tables.
Name a few examples of unstructured file types (HINT: the 90s tracksuit file is unstructured data)
Describe the function of Snowflake Stage Objects. (e.g. Is it a location? Is it temporary?)


 Query Data in the ZMD stage
dot copy symbol

select $1
from @uni_klaus_zmd; 

Snowflake hasn't been told anything about how the data in these files is structured so it's just making assumptions.  Snowflake is presuming that the files are CSVs because CSVs are a very popular file-formatting choice. It's also presuming each row ends with CRLF (Carriage Return Line Feed) because CRLF is also very common as a row delimiter.
Snowflake hedges its bets and presumes if you don't tell it anything about your file, the file is probably a standard CSV.
By using these assumptions, Snowflake treats the product_coordination_suggestions.txt file as if it only has one column and one row. 





create file format zmd_file_format_3
FIELD_DELIMITER = '='
RECORD_DELIMITER = '^'; 

select $1, $2
from @uni_klaus_zmd/product_coordination_suggestions.txt
(file_format => zmd_file_format_3);





create or replace file format zmd_file_format_3
FIELD_DELIMITER = '='
RECORD_DELIMITER = '^'; 


When we talk about Data Lakes in Snowflake, what do we mean?
Data that is left outside of Snowflake but can be accessed using Snowflake tools.



 "Data Lake." The Data Lake metaphor was introduced to the world in 2011 by James Dixon, who was the Chief Technology Officer for a company called Pentaho, at that time.

Dixon said:

If you think of a data mart as a store of bottled water -- cleansed and packaged and structured for easy consumption -- the data lake is a large body of water in a more natural state. The contents of the data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.
When we talk about Data Lakes at Snowflake, we tend to mean data that has not been loaded into traditional Snowflake tables. We might also call these traditional tables "native" Snowflake tables, or "regular" tables. 

As we've already seen, Structured and Semi-structured data that is sitting outside of Snowflake can be easily accessed and analyzed using familiar Snowflake tools like views, file formats, and SQL queries. 

We've also seen how Unstructured data, not loaded into Snowflake, can be accessed with a special Snowflake tool called a Directory Table. We've also seen how Directory Tables can be used in combination with functions, joins, internal tables, and standard views to access that non-loaded data. 

And through all this, we've seen that bringing together loaded and non-loaded data is a simple and seamless process.  When some data is loaded and some is left in a non-loaded state the two types can be joined and queried together, this is sometimes referred to as a Data Lakehouse. 



data lakes focuses more specifically on one of the WHEREs. In this case, the WHERE is external. The WHERE is external to Snowflake's native tables. 

https://docs.snowflake.com/en/sql-reference/sql/create-file-format
create file format ff_json
type="JSON";



select get_ddl('view', 'DENVER_AREA_TRAILS');

-- Melanie's Location into a 2 Variables (mc for melanies cafe)
set mc_lat='-104.97300245114094';
set mc_lng='39.76471253574085';


--Filling in the Function Code
CREATE or replace FUNCTION distance_to_mc(loc_lat number(38,32), loc_lng number(38,32))
  RETURNS FLOAT
  AS
  $$
    st_distance(
        st_makepoint('-104.97300245114094','39.76471253574085')
        ,st_makepoint(loc_lat,loc_lng)
        )
  $$
  ;



A Materialized View is like a view that is frozen in place (more or less looks and acts like a table).
The big difference is that if some part of the underlying data changes,  Snowflake recognizes the need to refresh it, automatically.
People often choose to create a materialized view if they have a view with intensive logic that they query often but that does NOT change often.  We can't use a Materialized view on any of our trails data because you can't put a materialized view directly on top of staged data. 



An External Table is a table put over the top of non-loaded data (sounds like our recent views, right?).
An External Table points at a stage folder(yep, we know how to do that!) and includes a reference to a file format (or formatting attributes) much like what we've been doing with our views for most of this workshop! Seems very straightforward and something within reach-- given what we've already learned in this workshop!




Which of these can be placed directly on top of a stage?
An External Table yes
A "regular" View yes
A Materialized View no



create secure materialized view SMV_CHERRY_CREEK_TRAIL
    -- comment = '<comment>'
    as select * from T_CHERRY_CREEK_TRAIL ;


Well, we left out an important detail. You CAN put a Materialized View over an External Table, even if that External Table is based on a Stage!!
In other words, you CAN put a Materialized View over staged data, as long as you put an External Table in between them, first!




Iceberg tables are coming to Snowflake!! When? We don't want to say for sure, because it might change. You can't get hands on experience with Iceberg tables in this workshop (yet) but you won't want a Snowflake Data Lake Badge that didn't hype you up about Iceberg Tables and their enormous potential.

You'll at least want to know what they are, and what they will make possible in Snowflake!! 

Iceberg is an open-source table type, which means a private company does not own the technology. Iceberg Table technology is not proprietary. 
Iceberg Tables are a layer of functionality you can lay on top of parquet files (just like the Cherry Creek Trails file we've been using) that will make files behave more like loaded data. In this way, it's like a file format, but also MUCH more. 
Iceberg Table data will be editable via Snowflake! Read that again. Not just the tables are editable (like the table name), but the data they make available (like the data values in columns and rows). So, you will be able to create an Iceberg Table in Snowflake, on top of a set of parquet files that have NOT BEEN LOADED into Snowflake, and then run INSERT and UPDATE statements on the data using SQL 🤯. 
Iceberg Tables will make Snowflake's Data Lake options incredibly powerful!!

THIS CHANGES EVERYTHING

People sometimes think of Snowflake as a solution for structured, normalized data (which they often call a Data Warehouse). For a while, people said Data Lakes were the only path forward. Lately, many people say the best solution is a Data Lakehouse (they're just mushing the two terms together and saying you need both).




Below, we've linked to an article about Iceberg tables in Snowflake. It gives more history on Iceberg tables and more technical details. It may be a bit advanced so don't worry if you can't process all of what is being said. 
https://www.snowflake.com/blog/5-reasons-apache-iceberg/
You may notice a nuance here.  The nuance is this - first Snowflake announced they would have an Iceberg FORMAT as a type of Snowflake External table. That use of Iceberg would not allow updates and inserts.
Later, Snowflake announced Iceberg tables as a new kind of object, not a type of External Table, but it's own thing. The whole-new-thing Iceberg tables are the ones that will enable updates and inserts. 



There are many great use cases for non-loaded data:
Rapid prototyping and proof of concept development (like we saw with Zena and Mel's projects)
Unstructured Data that cannot be stored in traditional tables (like Zena needed in order to access the clothing images)
Data used for mining and exploration by Data Scientists in prototyping possible AI/ML Models
Data that cannot be moved by a company because of a regulatory restrictions



L2: Which file types below are considered STRUCTURED?
Select 3.
Comma Separated (CSV)
Tab Separated (TSV)
Pipe-delimited and named *.txt


L2: Which file types below are considered SEMI-STRUCTURED?
Select 6.
JSON formatted and named *.json
JSON formatted and named *.txt
Avro
Parquet
XML
ORC



L2: Which file types or data types below are considered UNSTRUCTURED?
Select 4.
Images
Videos
PDFs
Audio


alter user rhender5 set default_role = 'SYSADMIN';
alter user rhender5 set default_warehouse = 'COMPUTE_WH';
alter user rhender5 set default_namespace = 'DEMO_DB.PUBLIC';

data engineer use sysadmin role for almost everything


A data warehouse typically stores data in a predetermined organization with a schema. A data lake does not have a predetermined schema. Also, whereas a data warehouse usually stores structured data, a data lake stores structured and unstructured data. And a data warehouse, especially one where storage and compute workloads are separated by design, delivers far faster analytics and much higher concurrency.



When writing a COPY INTO statement, what happens if you only use the name of the stage (or stage/folder) on the FROM line?
Snowflake will attempt to load all the files in the stage (or the stage/folder location)


Every Snowflake Account's time zone is initially set to UTC-7. What do we suggest is the reason for that?
Because that's the time zone for San Mateo, California, USA.


--worksheets are sometimes called sessions -- we'll be changing the worksheet time zone
alter session set timezone = 'UTC';
select current_timestamp();

Other courses and information sources are available and may be a better fit for your needs and preferences. 

For non-hands on courses and certification preparation guides, see other courses on this site. 
For advanced tutorials, go to quickstarts.snowflake.com 
For instructor-led training go to training.snowflake.com
For documentation go to docs.snowflake.com
For forum discussions and technical support go to community.snowflake.com 
